- metrics, visualization -> tensorboard
- confusion matrix
    - compare cnn and lstm
- manual training loop
- dataset split
    - sliding window -> create test data from users 27-30 and train from 1-26
    - (include unlabeled data)
    - check numbers of classes in test and train set, distribution of classes
    - Validation set
- number of samples per class
    - oversample transitions
- bidirectional lstm
- Presentation Power point
- comparison regarding time, accuracy
    - try different optimizers, architectures, loss functions, dynamic learning rate, initialization, different hyperparameters
- transfer learning, few-shot learning
    - balancing the data

- Transformer, GRU
- pros and cons between different models
- relationship between performance and model complexity
- try to use the 561 extracted features instead of raw data -> SVM, DNN -> pros and cons

Done:
- LSTM tensorflow
- improve LSTM, CNN LSTM
- understand data generation



Project hints
Accuracy is not everything, you can use the features extracted by UCI HAR dataset (e.g. max, min values) and conventional machine learning algorihtm (e.g. SVM, logistic regression). Recording your findings and analyzing the pros and cons for both sides is a good direction to think about.

There are more sophisticated deep learning models available for time-series data (e.g. LSTM, transformer). Investigating the pros and cons between different models is very interesting.

For the same algorithm, e.g. 1D CNN, different hyperparameter setting leads to different levels of model complexity. You can investigate the relationship between model performance and model complexity, what can you notice? and try to explain why.